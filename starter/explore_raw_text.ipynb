{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642f67ab",
   "metadata": {},
   "source": [
    "# Obtaining and Exploring a Real-World Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce4198",
   "metadata": {},
   "source": [
    "In industry, you almost never start with a perfectly curated list of documents.\n",
    "You start by:\n",
    "\n",
    "- locating an existing corpus\n",
    "\n",
    "- understanding its scope and structure\n",
    "\n",
    "- checking what it contains and what it does not\n",
    "\n",
    "- inspecting it before deciding how to preprocess it\n",
    "\n",
    "https://stateoftheunion.onetwothree.net/\n",
    "\n",
    "https://www.kaggle.com/datasets/nltkdata/state-union-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fca19",
   "metadata": {},
   "source": [
    "## Loading a Real Corpus (State of the Union)\n",
    "\n",
    "NLTK includes a built-in corpus of all U.S. State of the Union addresses spanning more than two centuries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5680414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt_tab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef0a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1bf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01ed452",
   "metadata": {},
   "source": [
    "- Each file corresponds to one speech, typically named by year and president.\n",
    "- Each file is a document.\n",
    "- The full collection is a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eeb0cb",
   "metadata": {},
   "source": [
    "## Understanding Corpus Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d58ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2ff87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9751b443",
   "metadata": {},
   "source": [
    "- This corpus spans over 200 years of political language.\n",
    "- This makes it suitable for temporal, historical, and policy trend analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c843b9",
   "metadata": {},
   "source": [
    "# Inspecting Raw Text (No Processing Yet)\n",
    "Before tokenization, normalization, or cleaning, we inspect the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc931bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d8a7ba1",
   "metadata": {},
   "source": [
    "- Is this clean or messy text?\n",
    "\n",
    "- Do you see formatting artifacts?\n",
    "\n",
    "- Would you need domain-specific cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ff19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 500 words of the 1945 Truman address\n",
    "truman_1945_words = state_union.words('1945-Truman.txt')[:500]\n",
    "#truman_1945_text = ' '.join(truman_1945_words)  # Join words into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983b5307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRESIDENT',\n",
       " 'HARRY',\n",
       " 'S',\n",
       " '.',\n",
       " 'TRUMAN',\n",
       " \"'\",\n",
       " 'S',\n",
       " 'ADDRESS',\n",
       " 'BEFORE',\n",
       " 'A',\n",
       " 'JOINT',\n",
       " 'SESSION',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'CONGRESS',\n",
       " 'April',\n",
       " '16',\n",
       " ',',\n",
       " '1945',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Speaker',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'President',\n",
       " ',',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Congress',\n",
       " ':',\n",
       " 'It',\n",
       " 'is',\n",
       " 'with',\n",
       " 'a',\n",
       " 'heavy',\n",
       " 'heart',\n",
       " 'that',\n",
       " 'I',\n",
       " 'stand',\n",
       " 'before',\n",
       " 'you',\n",
       " ',',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'and',\n",
       " 'colleagues',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Congress',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " '.',\n",
       " 'Only',\n",
       " 'yesterday',\n",
       " ',',\n",
       " 'we',\n",
       " 'laid',\n",
       " 'to',\n",
       " 'rest',\n",
       " 'the',\n",
       " 'mortal',\n",
       " 'remains',\n",
       " 'of',\n",
       " 'our',\n",
       " 'beloved',\n",
       " 'President',\n",
       " ',',\n",
       " 'Franklin',\n",
       " 'Delano',\n",
       " 'Roosevelt',\n",
       " '.',\n",
       " 'At',\n",
       " 'a',\n",
       " 'time',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'words',\n",
       " 'are',\n",
       " 'inadequate',\n",
       " '.',\n",
       " 'The',\n",
       " 'most',\n",
       " 'eloquent',\n",
       " 'tribute',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'reverent',\n",
       " 'silence',\n",
       " '.',\n",
       " 'Yet',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'decisive',\n",
       " 'hour',\n",
       " ',',\n",
       " 'when',\n",
       " 'world',\n",
       " 'events',\n",
       " 'are',\n",
       " 'moving',\n",
       " 'so',\n",
       " 'rapidly',\n",
       " ',',\n",
       " 'our',\n",
       " 'silence',\n",
       " 'might',\n",
       " 'be',\n",
       " 'misunderstood',\n",
       " 'and',\n",
       " 'might',\n",
       " 'give',\n",
       " 'comfort',\n",
       " 'to',\n",
       " 'our',\n",
       " 'enemies',\n",
       " '.',\n",
       " 'In',\n",
       " 'His',\n",
       " 'infinite',\n",
       " 'wisdom',\n",
       " ',',\n",
       " 'Almighty',\n",
       " 'God',\n",
       " 'has',\n",
       " 'seen',\n",
       " 'fit',\n",
       " 'to',\n",
       " 'take',\n",
       " 'from',\n",
       " 'us',\n",
       " 'a',\n",
       " 'great',\n",
       " 'man',\n",
       " 'who',\n",
       " 'loved',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'beloved',\n",
       " 'by',\n",
       " ',',\n",
       " 'all',\n",
       " 'humanity',\n",
       " '.',\n",
       " 'No',\n",
       " 'man',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'tremendous',\n",
       " 'void',\n",
       " 'left',\n",
       " 'by',\n",
       " 'the',\n",
       " 'passing',\n",
       " 'of',\n",
       " 'that',\n",
       " 'noble',\n",
       " 'soul',\n",
       " '.',\n",
       " 'No',\n",
       " 'words',\n",
       " 'can',\n",
       " 'ease',\n",
       " 'the',\n",
       " 'aching',\n",
       " 'hearts',\n",
       " 'of',\n",
       " 'untold',\n",
       " 'millions',\n",
       " 'of',\n",
       " 'every',\n",
       " 'race',\n",
       " ',',\n",
       " 'creed',\n",
       " 'and',\n",
       " 'color',\n",
       " '.',\n",
       " 'The',\n",
       " 'world',\n",
       " 'knows',\n",
       " 'it',\n",
       " 'has',\n",
       " 'lost',\n",
       " 'a',\n",
       " 'heroic',\n",
       " 'champion',\n",
       " 'of',\n",
       " 'justice',\n",
       " 'and',\n",
       " 'freedom',\n",
       " '.',\n",
       " 'Tragic',\n",
       " 'fate',\n",
       " 'has',\n",
       " 'thrust',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'grave',\n",
       " 'responsibilities',\n",
       " '.',\n",
       " 'We',\n",
       " 'must',\n",
       " 'carry',\n",
       " 'on',\n",
       " '.',\n",
       " 'Our',\n",
       " 'departed',\n",
       " 'leader',\n",
       " 'never',\n",
       " 'looked',\n",
       " 'backward',\n",
       " '.',\n",
       " 'He',\n",
       " 'looked',\n",
       " 'forward',\n",
       " 'and',\n",
       " 'moved',\n",
       " 'forward',\n",
       " '.',\n",
       " 'That',\n",
       " 'is',\n",
       " 'what',\n",
       " 'he',\n",
       " 'would',\n",
       " 'want',\n",
       " 'us',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'That',\n",
       " 'is',\n",
       " 'what',\n",
       " 'America',\n",
       " 'will',\n",
       " 'do',\n",
       " '.',\n",
       " 'So',\n",
       " 'much',\n",
       " 'blood',\n",
       " 'has',\n",
       " 'already',\n",
       " 'been',\n",
       " 'shed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'ideals',\n",
       " 'which',\n",
       " 'we',\n",
       " 'cherish',\n",
       " ',',\n",
       " 'and',\n",
       " 'for',\n",
       " 'which',\n",
       " 'Franklin',\n",
       " 'Delano',\n",
       " 'Roosevelt',\n",
       " 'lived',\n",
       " 'and',\n",
       " 'died',\n",
       " ',',\n",
       " 'that',\n",
       " 'we',\n",
       " 'dare',\n",
       " 'not',\n",
       " 'permit',\n",
       " 'even',\n",
       " 'a',\n",
       " 'momentary',\n",
       " 'pause',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hard',\n",
       " 'fight',\n",
       " 'for',\n",
       " 'victory',\n",
       " '.',\n",
       " 'Today',\n",
       " ',',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'world',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'America',\n",
       " 'for',\n",
       " 'enlightened',\n",
       " 'leadership',\n",
       " 'to',\n",
       " 'peace',\n",
       " 'and',\n",
       " 'progress',\n",
       " '.',\n",
       " 'Such',\n",
       " 'a',\n",
       " 'leadership',\n",
       " 'requires',\n",
       " 'vision',\n",
       " ',',\n",
       " 'courage',\n",
       " 'and',\n",
       " 'tolerance',\n",
       " '.',\n",
       " 'It',\n",
       " 'can',\n",
       " 'be',\n",
       " 'provided',\n",
       " 'only',\n",
       " 'by',\n",
       " 'a',\n",
       " 'united',\n",
       " 'nation',\n",
       " 'deeply',\n",
       " 'devoted',\n",
       " 'to',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'ideals',\n",
       " '.',\n",
       " 'With',\n",
       " 'great',\n",
       " 'humility',\n",
       " 'I',\n",
       " 'call',\n",
       " 'upon',\n",
       " 'all',\n",
       " 'Americans',\n",
       " 'to',\n",
       " 'help',\n",
       " 'me',\n",
       " 'keep',\n",
       " 'our',\n",
       " 'nation',\n",
       " 'united',\n",
       " 'in',\n",
       " 'defense',\n",
       " 'of',\n",
       " 'those',\n",
       " 'ideals',\n",
       " 'which',\n",
       " 'have',\n",
       " 'been',\n",
       " 'so',\n",
       " 'eloquently',\n",
       " 'proclaimed',\n",
       " 'by',\n",
       " 'Franklin',\n",
       " 'Roosevelt',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'to',\n",
       " 'assure',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'Americans',\n",
       " 'and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'who',\n",
       " 'love',\n",
       " 'peace',\n",
       " 'and',\n",
       " 'liberty',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'world',\n",
       " 'that',\n",
       " 'I',\n",
       " 'will',\n",
       " 'support',\n",
       " 'and',\n",
       " 'defend',\n",
       " 'those',\n",
       " 'ideals',\n",
       " 'with',\n",
       " 'all',\n",
       " 'my',\n",
       " 'strength',\n",
       " 'and',\n",
       " 'all',\n",
       " 'my',\n",
       " 'heart',\n",
       " '.',\n",
       " 'That',\n",
       " 'is',\n",
       " 'my',\n",
       " 'duty',\n",
       " 'and',\n",
       " 'I',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'shirk',\n",
       " 'it',\n",
       " '.',\n",
       " 'So',\n",
       " 'that',\n",
       " 'there',\n",
       " 'can',\n",
       " 'be',\n",
       " 'no',\n",
       " 'possible',\n",
       " 'misunderstanding',\n",
       " ',',\n",
       " 'both',\n",
       " 'Germany',\n",
       " 'and',\n",
       " 'Japan',\n",
       " 'can',\n",
       " 'be',\n",
       " 'certain',\n",
       " ',',\n",
       " 'beyond',\n",
       " 'any',\n",
       " 'shadow',\n",
       " 'of',\n",
       " 'a',\n",
       " 'doubt',\n",
       " ',',\n",
       " 'that',\n",
       " 'America',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'the',\n",
       " 'fight',\n",
       " 'for',\n",
       " 'freedom',\n",
       " 'until',\n",
       " 'no',\n",
       " 'vestige',\n",
       " 'of',\n",
       " 'resistance',\n",
       " 'remains',\n",
       " '!',\n",
       " 'We',\n",
       " 'are',\n",
       " 'deeply',\n",
       " 'conscious',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'much',\n",
       " 'hard',\n",
       " 'fighting',\n",
       " 'is',\n",
       " 'still',\n",
       " 'ahead',\n",
       " 'of',\n",
       " 'us',\n",
       " '.',\n",
       " 'Having',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'such',\n",
       " 'a',\n",
       " 'heavy',\n",
       " 'price',\n",
       " 'to',\n",
       " 'make',\n",
       " 'complete',\n",
       " 'victory',\n",
       " 'certain',\n",
       " ',',\n",
       " 'America',\n",
       " 'will',\n",
       " 'never',\n",
       " 'become',\n",
       " 'a',\n",
       " 'party',\n",
       " 'to',\n",
       " 'any',\n",
       " 'plan',\n",
       " 'for',\n",
       " 'partial',\n",
       " 'victory',\n",
       " '!',\n",
       " 'To',\n",
       " 'settle',\n",
       " 'for',\n",
       " 'merely',\n",
       " 'another',\n",
       " 'temporary',\n",
       " 'respite',\n",
       " 'would',\n",
       " 'surely',\n",
       " 'jeopardize']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truman_1945_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7679c6",
   "metadata": {},
   "source": [
    "# Words vs Sentences vs Characters?\n",
    "At this stage, we only explore structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea768b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73a1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55a7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710c44f8",
   "metadata": {},
   "source": [
    "# Quick Corpus-Wide Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c7578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "954713eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\david\\aig230-env\\lib\\site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\david\\aig230-env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.1.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\david\\aig230-env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david\\aig230-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  8.1/8.1 MB 45.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 38.8 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 44.0 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Downloading pillow-12.1.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 7.0/7.0 MB 36.2 MB/s  0:00:00\n",
      "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe0f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d47479",
   "metadata": {},
   "source": [
    "In the next section, we will treat each State of the Union address as a document and the full collection as a corpus. We will begin designing preprocessing pipelines using both NLTK and spaCy, and we will compare how each library handles the same steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8cc9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIG230)",
   "language": "python",
   "name": "aig230-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
