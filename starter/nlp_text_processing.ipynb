{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f6469b",
   "metadata": {},
   "source": [
    "# AIG 230 ‚Äì Week 2 Lab\n",
    "## From Raw Text to Corpus: Tokenization, Normalization, and Vocabulary\n",
    "\n",
    "Industry Context: Exploring the State of the Union Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d1b9c",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand raw text, documents, and corpora\n",
    "- Explore a real-world corpus\n",
    "- Compare NLTK and spaCy preprocessing pipelines\n",
    "- Perform tokenization, normalization, and vocabulary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a390dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"state_union\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1368c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 16.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 29.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 32.2 MB/s  0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "# Run once if needed:\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6847",
   "metadata": {},
   "source": [
    "Breaking it down:\n",
    "\n",
    "```\n",
    "spacy.load() - loads a pre-trained language model\n",
    "```\n",
    "\n",
    "\"en_core_web_sm\" - the small English model that includes:\n",
    "\n",
    "- Tokenizer - splits text into words/sentences\n",
    "- Part-of-speech tagger - identifies noun, verb, adjective, etc.\n",
    "- Dependency parser - analyzes grammatical relationships\n",
    "- Named entity recognizer - identifies people, places, organizations\n",
    "- Word vectors - semantic meaning of words\n",
    "\n",
    "nlp = - stores the loaded model in a variable so you can use it to process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d0066",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äì Obtaining the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa16697e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1945-Truman.txt',\n",
       " '1946-Truman.txt',\n",
       " '1947-Truman.txt',\n",
       " '1948-Truman.txt',\n",
       " '1949-Truman.txt',\n",
       " '1950-Truman.txt',\n",
       " '1951-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1954-Eisenhower.txt',\n",
       " '1955-Eisenhower.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import state_union\n",
    "state_union.fileids()[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d7ee5",
   "metadata": {},
   "source": [
    "Each file is a document. The collection is the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e529d0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(state_union.fileids())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc584e2",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äì Inspect Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11505dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PRESIDENT HARRY S. TRUMAN'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS\\n \\nApril 16, 1945\\n\\nMr. Speaker, Mr. President, Members of the Congress:\\nIt is with a heavy heart that I stand before you, my friends and colleagues, in the Congress of the United States.\\nOnly yesterday, we laid to rest the mortal remains of our beloved President, Franklin Delano Roosevelt. At a time like this, words are inadequate. The most eloquent tribute would be a reverent silence.\\nYet, in this decisive hour, when world events are moving so rapidly, our silence might be misunderstood and might give comfort to our enemies.\\nIn His infinite wisdom, Almighty God has seen fit to take from us a great man who loved, and was beloved by, all humanity.\\nNo man could possibly fill the tremendous void left by the passing of that noble soul. No words can ease the aching hearts of untold millions of every race, creed and color. The world knows it has lost a heroic champion of justice and freedom.\\nTragic fate has thrust upon\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_file = state_union.fileids()[0]\n",
    "raw_text = state_union.raw(sample_file)\n",
    "raw_text[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172ef1a",
   "metadata": {},
   "source": [
    "## Part 3 ‚Äì Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b030c7",
   "metadata": {},
   "source": [
    "üìå Tokenization splits text into meaningful units (tokens).\n",
    "There is no universal standard, but conventions vary by task and language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988b99e",
   "metadata": {},
   "source": [
    "## Part 4 ‚Äì Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e8fd7",
   "metadata": {},
   "source": [
    "## Part 5 ‚Äì Normalization\n",
    "Normalization makes text more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b62c0",
   "metadata": {},
   "source": [
    "## Part 6 ‚Äì Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d074b38",
   "metadata": {},
   "source": [
    "üìå Stop words are common words that often add little semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438fcbe",
   "metadata": {},
   "source": [
    "## Part 7 ‚Äì Vocabulary and Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4acb8d",
   "metadata": {},
   "source": [
    "üìå The vocabulary is the set of unique tokens in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4f2f9",
   "metadata": {},
   "source": [
    "# Stemming vs Lemmatization\n",
    "\n",
    "Stemming and lemmatization are both normalization techniques, but they make very different trade-offs.\n",
    "\n",
    "Stemming is fast and rule-based but can distort meaning\n",
    "\n",
    "Lemmatization is slower but linguistically informed\n",
    "\n",
    "In industry, the choice depends on task, domain, and interpretability requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1cce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a72b89db",
   "metadata": {},
   "source": [
    "‚Äúdemocracy‚Äù ‚Üí ‚Äúdemocraci‚Äù\n",
    "\n",
    "stems are not necessarily real words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da72d7",
   "metadata": {},
   "source": [
    "### spaCy does not include a built-in stemmer by default.\n",
    "\n",
    "This is not a limitation. It is a design choice.\n",
    "\n",
    "spaCy prioritizes:\n",
    "\n",
    "- linguistically informed processing\n",
    "\n",
    "- lemmatization over stemming\n",
    "\n",
    "However, in real pipelines, you can still perform stemming alongside spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d51096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a190ce",
   "metadata": {},
   "source": [
    "NLTK does support lemmatization, but it requires:\n",
    "\n",
    "- a lemmatizer\n",
    "\n",
    "- part-of-speech information to work well\n",
    "\n",
    "By default, NLTK‚Äôs lemmatizer assumes nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61039dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c709b50",
   "metadata": {},
   "source": [
    "Notice that many verbs are not lemmatized correctly.\n",
    "This is because NLTK‚Äôs lemmatizer defaults to noun POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2fd8059",
   "metadata": {},
   "source": [
    "# From Words to Subwords: Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98784c3c",
   "metadata": {},
   "source": [
    "So far, we have treated words as the basic unit of meaning.\n",
    "Modern NLP systems often go one step further and operate on subword units.\n",
    "\n",
    "One of the most common subword tokenization methods is Byte Pair Encoding (BPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb140f",
   "metadata": {},
   "source": [
    "In large corpora like the State of the Union addresses, word-level tokenization creates several problems:\n",
    "\n",
    "Rare words appear very infrequently\n",
    "\n",
    "New words appear over time (e.g. cybersecurity, biotechnology)\n",
    "\n",
    "Related words are treated as completely separate tokens\n",
    "\n",
    "Subword tokenization solves this by breaking words into frequently occurring pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c59ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['democracy',\n",
       " 'democratic',\n",
       " 'democratization',\n",
       " 'economy',\n",
       " 'economic',\n",
       " 'economics']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use a small subset of real policy-related words that appear in State of the Union speeches.\n",
    "\n",
    "words = [\n",
    "    \"democracy\",\n",
    "    \"democratic\",\n",
    "    \"democratization\",\n",
    "    \"economy\",\n",
    "    \"economic\",\n",
    "    \"economics\"\n",
    "]\n",
    "\n",
    "words\n",
    "# At the word level, all of these are treated as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1122e16",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äì Character-Level Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f2b25",
   "metadata": {},
   "source": [
    "BPE starts by representing each word as a sequence of characters\n",
    "(with a special end-of-word marker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3127d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['d', 'e', 'm', 'o', 'c', 'r', 'a', 'c', 'y', '</w>'],\n",
       " ['d', 'e', 'm', 'o', 'c', 'r', 'a', 't', 'i', 'c', '</w>'],\n",
       " ['d',\n",
       "  'e',\n",
       "  'm',\n",
       "  'o',\n",
       "  'c',\n",
       "  'r',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  '</w>'],\n",
       " ['e', 'c', 'o', 'n', 'o', 'm', 'y', '</w>'],\n",
       " ['e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', '</w>'],\n",
       " ['e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 's', '</w>']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokens = [list(word) + [\"</w>\"] for word in words]\n",
    "char_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80846fee",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äì Count Frequent Character Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70412b",
   "metadata": {},
   "source": [
    "BPE repeatedly merges the most frequent adjacent character pairs across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47f9287c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('o', 'n'), 4),\n",
       " (('d', 'e'), 3),\n",
       " (('e', 'm'), 3),\n",
       " (('m', 'o'), 3),\n",
       " (('o', 'c'), 3),\n",
       " (('c', 'r'), 3),\n",
       " (('r', 'a'), 3),\n",
       " (('a', 't'), 3),\n",
       " (('t', 'i'), 3),\n",
       " (('i', 'c'), 3)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pair_counts = Counter()\n",
    "\n",
    "for word in char_tokens:\n",
    "    for i in range(len(word) - 1):\n",
    "        pair = (word[i], word[i+1])\n",
    "        pair_counts[pair] += 1\n",
    "\n",
    "pair_counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbf90b",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äì Merge Frequent Pairs (Conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dd120",
   "metadata": {},
   "source": [
    "The most frequent pair is ('o', 'n').\n",
    "BPE merges it into a new token: \"on\".\n",
    "\n",
    "This process repeats many times, gradually forming meaningful subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bad9d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['democr', 'acy</w>'],\n",
       " ['democr', 'atic</w>'],\n",
       " ['democr', 'atization</w>'],\n",
       " ['econ', 'omy</w>'],\n",
       " ['econ', 'omic</w>'],\n",
       " ['econ', 'omics</w>']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokens_example = [\n",
    "    [\"democr\", \"acy</w>\"],\n",
    "    [\"democr\", \"atic</w>\"],\n",
    "    [\"democr\", \"atization</w>\"],\n",
    "    [\"econ\", \"omy</w>\"],\n",
    "    [\"econ\", \"omic</w>\"],\n",
    "    [\"econ\", \"omics</w>\"]\n",
    "]\n",
    "\n",
    "bpe_tokens_example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIG230)",
   "language": "python",
   "name": "aig230-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
